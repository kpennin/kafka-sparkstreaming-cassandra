{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21`\n",
    "\n",
    "import $ivy.`org.apache.kafka::kafka:0.10.0.0`\n",
    "import $ivy.`org.apache.spark::spark-core:2.0.0`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.0.0`\n",
    "import $ivy.`org.apache.spark::spark-streaming:2.0.0`\n",
    "import $ivy.`org.apache.spark::spark-streaming-kafka-0-10:2.0.0`\n",
    "import $ivy.`com.datastax.spark::spark-cassandra-connector:2.0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "\n",
    "import org.apache.spark.streaming.kafka010.{KafkaUtils, HasOffsetRanges, OffsetRange }\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "import org.apache.spark.streaming.{StreamingContext, Duration, Seconds}\n",
    "\n",
    "import org.apache.spark.streaming.dstream.InputDStream\n",
    "\n",
    "import org.apache.spark.{SparkContext, SparkConf, TaskContext}\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import com.datastax.spark.connector._\n",
    "\n",
    "import java.net.InetAddress\n",
    "import java.text.SimpleDateFormat\n",
    "import java.util.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val host_ip = InetAddress.getLocalHost().getHostAddress()\n",
    "\n",
    "val conf = new SparkConf().setAppName(\"Streaming test\").setMaster(\"local[1]\").set(\"spark.cassandra.connection.host\", host_ip)\n",
    "val sc = new SparkContext(conf)\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "val ssc = new StreamingContext(sc, Seconds(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val topics = Set(\"test\")\n",
    "val kafkaParams= scala.collection.immutable.Map[String, Object](\n",
    "        \"bootstrap.servers\" -> \"localhost:9092,127.0.0.1:2181\",\n",
    "        \"key.deserializer\" -> \"org.apache.kafka.common.serialization.StringDeserializer\",\n",
    "        \"value.deserializer\" -> \"org.apache.kafka.common.serialization.StringDeserializer\",\n",
    "        \"group.id\" -> \"spark-streaming-consumer\",\n",
    "//         \"auto.commit.interval.ms\" -> \"1000\",\n",
    "        \"enable.auto.commit\" -> (false: java.lang.Boolean),\n",
    "        \"auto.offset.reset\" -> \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val stream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](\n",
    "      ssc,\n",
    "      PreferConsistent,\n",
    "      Subscribe[String, String](topics, kafkaParams)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rows = stream.map(record => Row(record.value, new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(new Date())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rows.foreachRDD { rdd =>\n",
    "    if (!rdd.isEmpty()) {\n",
    "        val struct = StructType(\n",
    "            StructField(\"time_sent\", StringType, true) ::\n",
    "            StructField(\"time_received\", StringType, true) :: Nil\n",
    "        )\n",
    "        \n",
    "        sqlContext.createDataFrame(rdd, struct)\n",
    "                  .write\n",
    "                  .format(\"org.apache.spark.sql.cassandra\")\n",
    "                  .mode(\"append\")\n",
    "                  .options(scala.collection.Map(\"table\" -> \"sent_received\", \"keyspace\" -> \"test_time\"))\n",
    "                  .save()\n",
    "        //rdd.saveToCassandra(\"test_time\", \"sent_received\", SomeColumns(\"time_sent\", \"time_received\"))\n",
    "    }\n",
    "}\n",
    "\n",
    "rows.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
